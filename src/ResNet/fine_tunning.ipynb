{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import cv2\n",
    "from typing import List\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import models, transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "train_split = 0.7\n",
    "image_size = (224, 224)\n",
    "num_workers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "datasets_info = {\n",
    "    \"WLASL\": {\"name\": \"WLASL_frames_100\", \"path\": \"WLASL/frames_100\"},\n",
    "    \"animals\": {\"name\": \"animals_simple\", \"path\": \"animals/simple\"},\n",
    "}\n",
    "\n",
    "\n",
    "dataset_name = \"WLASL\"\n",
    "data_dir = f\"../../data/{datasets_info[dataset_name]['path']}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input labels with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_labels = listdir(data_dir)\n",
    "input_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  plt.imshow(img)\n",
    "  plt.show()\n",
    "  \n",
    "def imshow(img):\n",
    "  img = img / 2 + 0.5 # unnormalize\n",
    "  npimg = img.numpy() # convert to numpy objects\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = input_labels[random.randint(0, len(input_labels) - 1)]\n",
    "label_images = listdir(data_dir + \"/\" + label)\n",
    "label_images\n",
    "\n",
    "print(label)\n",
    "\n",
    "image = cv2.imread(data_dir + \"/\" + label + \"/\" + label_images[0])\n",
    "show_image(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_dataset(data_dir, train_split=0.7, batch_size=32, image_size=224):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size-10),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "    classes = dataset.classes\n",
    "    dataset_len = len(dataset)\n",
    "\n",
    "    train_split = int(train_split * dataset_len)\n",
    "    tets_split = dataset_len - train_split\n",
    "    train_set, test_set = random_split(dataset, [train_split, tets_split])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_set, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to unbalanced labels 1: Random dataset split to solve unbalanced labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_dataset(data_dir, train_split=0.7, batch_size=32, image_size=224):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "    classes = dataset.classes\n",
    "    dataset_len = len(dataset)\n",
    "\n",
    "    ran_ind = torch.randperm(dataset_len)  # 1. random\n",
    "    # seq_ind = list(range(dataset_len))  # 2. sequential\n",
    "\n",
    "    indices = ran_ind\n",
    "    split = int(np.floor(train_split * dataset_len))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=test_sampler,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, classes = load_split_dataset(\n",
    "    data_dir,\n",
    "    train_split=train_split,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    ")\n",
    "\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "print(train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many images are in each loader for every label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_balance_status(loader):\n",
    "    class_count = {}\n",
    "\n",
    "    for _, targets in loader:\n",
    "        for target in targets:\n",
    "            label = classes[target]\n",
    "            if label not in class_count:\n",
    "                class_count[label] = 0\n",
    "            class_count[label] += 1\n",
    "\n",
    "    return sorted(class_count.items())\n",
    "\n",
    "\n",
    "train_class_count = check_balance_status(train_loader)\n",
    "test_class_count = check_balance_status(test_loader)\n",
    "\n",
    "pc_mean = 0\n",
    "\n",
    "for tr, te in zip(train_class_count, test_class_count):\n",
    "    tr_pc = tr[1] / (tr[1] + te[1]) * 100\n",
    "    pc_mean += tr_pc\n",
    "    print(f\"{tr[0]} - TR: {tr[1]} TS: {te[1]} - PC-TR: {tr_pc:.2f}%\")\n",
    "    \n",
    "pc_mean /= len(train_class_count)\n",
    "print(f\"Mean PC-TR: {pc_mean:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrain model & modify it.\n",
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze model and train only last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Train network.\n",
    "    costs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # Get data to cuda if possible.\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward.\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # backward.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent or adam step.\n",
    "            optimizer.step()\n",
    "\n",
    "        cost = sum(losses) / len(losses)\n",
    "        costs.append(cost)\n",
    "        print(f\"Cost at epoch {epoch + 1} is {cost:.5f}\")\n",
    "\n",
    "    return costs\n",
    "\n",
    "\n",
    "costs = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs):\n",
    "    # Plot cost.\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_costs(costs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = datasets_info[dataset_name][\"name\"]\n",
    "model_path = f\"../../models/resnet_{model_name}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model.\n",
    "torch.save(model, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model for acc test.\n",
    "model = torch.load(model_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model predicts.\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "            # Output both images to compare.\n",
    "            print(f\"Images for {i+1}\")\n",
    "            imshow(make_grid(x.cpu()))\n",
    "\n",
    "            print(f\"Predictions for batch {i+1} \")\n",
    "            print([classes[int(i)] for i in predictions])\n",
    "\n",
    "            print(f\"Ground truth for batch {i+1}\")\n",
    "            print([classes[int(i)] for i in y])\n",
    "            \n",
    "            print(\"---------------------------------\\n\\n\")\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on Training Set\")\n",
    "check_accuracy(train_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on Test Set\")\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check train and test on same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(loader, model, debug_label):\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        scores = model(images)\n",
    "        _, predictions = scores.max(1)\n",
    "\n",
    "        for i, (image, target) in enumerate(zip(images, targets)):\n",
    "            label = classes[target]\n",
    "            if label != debug_label:\n",
    "                continue\n",
    "\n",
    "            # Predict label for image.\n",
    "            prediction_id = predictions[i]\n",
    "            prediction = classes[prediction_id]\n",
    "\n",
    "            # Show image.\n",
    "            imshow(image.cpu())\n",
    "\n",
    "            print(f\"Prediction: {prediction}. Ground truth: {label}\")\n",
    "\n",
    "            return\n",
    "\n",
    "    return sorted(class_count.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_label = \"apple\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction for {debug_label} and train set\")\n",
    "predict_class(train_loader, model, debug_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction for {debug_label} and test set\")\n",
    "predict_class(test_loader, model, debug_label)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e56335990ab617e20d6145e4b9769f2a0c38ec2a51571f2cf9b60e2973b4d12"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
