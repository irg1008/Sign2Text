{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.video_dataset import VideoFrameDataset\n",
    "from config.dataset import get_dataset_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from common.config.torch_config import get_transform, unnormalize, device\n",
    "from common.utils.output import plot_tensor, plot_train_val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.const import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path, model_path = get_dataset_path(\n",
    "    dataset=\"WLASL/videos\", model_name=\"WLASL_tanh_8\"\n",
    ")\n",
    "multiple_transform = get_transform(IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoFrameDataset(\n",
    "    root_path=data_path,\n",
    "    transform=multiple_transform,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    num_segments=NUM_SEGMENTS,\n",
    "    frames_per_segment=FRAMES_PER_SEGMENT,\n",
    ")\n",
    "\n",
    "classes = dataset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, validation_loader = split_dataset(\n",
    "    dataset, train_split=0.70, validation_split=0.1, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader), len(validation_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard logger y writter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./tensorboard/logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de entrada de la red\n",
    "\n",
    "> Initial input = [BATCH_SIZE, NUMBER_OF_FRAMES, CHANNELS, HEIGHT, WIDTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(\n",
    "    tensor,\n",
    "    dims=(1, 2, 0),\n",
    "    nrow=FRAMES_PER_SEGMENT * NUM_SEGMENTS,\n",
    "    unnorm=True,\n",
    "    start_dim=0,\n",
    "    end_dim=1,\n",
    "):\n",
    "    flat = nn.Flatten(start_dim=start_dim, end_dim=end_dim)  # Flatten batch to plot.\n",
    "\n",
    "    flatted_tensor = flat(tensor)\n",
    "    grid = make_grid(flatted_tensor.cpu(), nrow=nrow)\n",
    "\n",
    "    if unnorm:\n",
    "        grid = unnormalize(grid)\n",
    "\n",
    "    plot_tensor(grid, dims)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de batch completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch, (ground_classes, ground_poses) = next(iter(train_loader))\n",
    "grid = plot_grid(first_batch)\n",
    "\n",
    "print(first_batch.shape)\n",
    "print(ground_classes.shape)\n",
    "print(ground_poses.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacemos log del grid en tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_image(f\"Example of full batch with {BATCH_SIZE} videos\", grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_video = 0\n",
    "frame_of_video = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muestra el tensor con un video completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = first_batch[n_video]\n",
    "_ = plot_grid(video, nrow=len(video), end_dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muesta un único frame del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = video[frame_of_video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ground_classes[n_video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes[target])\n",
    "grid = plot_grid(img, end_dim=0)\n",
    "writer.add_image(\"Example of image\", grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muestra la pose de salida para ese frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_poses = ground_poses[n_video]\n",
    "img_pose = video_poses[frame_of_video]\n",
    "POSES_PER_FRAME = img_pose.shape[0] * img_pose.shape[1]\n",
    "print(img_pose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_img_pose = img_pose * IMAGE_SIZE\n",
    "img_pose_transpose = norm_img_pose.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = img_pose_transpose[0], img_pose_transpose[1]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostramos la pose superpuesta sobre el frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unorm_img = unnormalize(img)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.imshow(unorm_img.permute(1, 2, 0))\n",
    "\n",
    "fig = plt.gcf()\n",
    "writer.add_figure(\"Example of image with pose\", fig)\n",
    "\n",
    "# fig.show()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pequeña prueba de una convolución\n",
    "\n",
    "Hacemos esto para ver la salida tras aplicar filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1, hidden_2 = 16, 32\n",
    "\n",
    "conv1 = nn.Conv3d(\n",
    "    FRAMES_PER_SEGMENT * NUM_SEGMENTS,\n",
    "    hidden_1,\n",
    "    kernel_size=(2, 3, 3),\n",
    "    stride=2,\n",
    "    padding=1,\n",
    ")\n",
    "conv2 = nn.Conv3d(hidden_1, hidden_2, kernel_size=3, stride=2, padding=1)\n",
    "relu = nn.LeakyReLU()\n",
    "batch_1 = nn.BatchNorm3d(hidden_1)\n",
    "batch_2 = nn.BatchNorm3d(hidden_2)\n",
    "\n",
    "x = first_batch\n",
    "\n",
    "x = conv1(x)\n",
    "x = relu(x)\n",
    "x = batch_1(x)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = conv2(x)\n",
    "x = relu(x)\n",
    "x = batch_2(x)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "_ = plot_grid(x, nrow=hidden_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobamos el estado de balanceo de los loaders del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nets.common.utils.balance import check_balance_status\n",
    "\n",
    "# print(check_balance_status(test_loader, classes))\n",
    "# print(check_balance_status(validation_loader, classes))\n",
    "# print(check_balance_status(train_loader, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model import CNN\n",
    "\n",
    "num_frames = FRAMES_PER_SEGMENT * NUM_SEGMENTS\n",
    "model = CNN(\n",
    "    num_classes=len(classes),\n",
    "    num_frames=num_frames,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    num_pose_points=POSES_PER_FRAME * num_frames,\n",
    ")\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, first_batch)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostramos un grafo del modelo con tensorboard\n",
    "\n",
    "<img src=\"https://i.imgur.com/cvkNqyB.png\" alt=\"Grafo modelo con tensorboard\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.train import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_costs, val_costs, train_accs, val_accs = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    writer=writer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot de pérdida y accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_data(train_costs, val_costs, ylabel=\"Costs\")\n",
    "plot_train_val_data(train_accs, val_accs, ylabel=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from common.utils.check_accuracy import check_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobamos el accuracy de la red en los tres sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(train_loader, model, classes, device, n_batchs=10, has_pose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(validation_loader, model, classes, device, has_pose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(test_loader, model, classes, device, has_pose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportamos modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save, onnx, randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model, model_path)\n",
    "print(f\"Model exported to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportamos modelo en formato estandar ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = randn(\n",
    "    BATCH_SIZE,\n",
    "    FRAMES_PER_SEGMENT * NUM_SEGMENTS,\n",
    "    3,\n",
    "    IMAGE_SIZE,\n",
    "    IMAGE_SIZE,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    model_path.replace(\".pth\", \".onnx\"),\n",
    "    input_names=[\"input\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostramos el grafo de onnx con [_netrón_](https://netron.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/jDkeBMz.png\" alt=\"Grafo del modelo exportado con netrón\" width=\"400\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f256e947ed30eec6960a657ce8e0e10d9b747cc7764d264741e6353e53d381f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
